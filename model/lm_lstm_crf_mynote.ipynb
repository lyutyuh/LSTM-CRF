{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "os.chdir('/mnt/E/WORK/NER/LM-LSTM-CRF/')\n",
    "import codecs\n",
    "import model.crf as crf\n",
    "import model.utils as utils\n",
    "import model.highway as highway\n",
    "from torch.nn import Embedding\n",
    "from torch.nn import Conv1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open(\"data/ner/dev.txt\", 'r', 'utf-8') as f:\n",
    "    dev_lines = f.readlines()\n",
    "    dev_features, dev_labels = utils.read_corpus(dev_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open(\"data/ner/train.txt\", 'r', 'utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    train_features, train_labels, f_map, l_map, c_map = utils.generate_corpus_char(lines, \n",
    "                                                           if_shrink_c_feature=True, \n",
    "                                                           c_thresholds=5, \n",
    "                                                           if_shrink_w_feature=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset, forw_dev, back_dev = utils.construct_bucket_mean_vb_wc(dev_features, dev_labels, l_map, c_map, f_map, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([811, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_dataset[0].len_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence_loader():\n",
    "    def __init__(self):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Dense character embedding.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_chars : int\n",
    "        The number of characters in the vocabularly, i.e. the input size.\n",
    "    embedding_size : int\n",
    "        The dimension of the embedding.\n",
    "    dropout : float, optional (default: 0.)\n",
    "        The dropout probability.\n",
    "    padding_idx : int, optional (default: 0)\n",
    "        The id of the character using for padding.\n",
    "    Attributes\n",
    "    ----------\n",
    "    n_chars : int\n",
    "        The number of characters in the vocabularly, i.e. the input size.\n",
    "    embedding : torch.nn.Embedding\n",
    "        The character embedding layer.\n",
    "    embedding_dropout : torch.nn.Dropout\n",
    "        A dropout applied to the embedding features.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 n_chars: int,\n",
    "                 embedding_size: int,\n",
    "                 dropout: float = 0.,\n",
    "                 padding_idx: int = 0) -> None:\n",
    "        super(CharEmbedding, self).__init__()\n",
    "        self.n_chars = n_chars\n",
    "        # Character embedding layer.\n",
    "        self.embedding = \\\n",
    "            Embedding(self.n_chars, embedding_size, padding_idx=padding_idx)\n",
    "        # Dropout applied to embeddings.\n",
    "        self.embedding_dropout = \\\n",
    "            Dropout(p=dropout) if dropout else None\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Make foward pass.\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : torch.Tensor\n",
    "            Tensor of shape ``[batch_size x sent_length x max_word_length]``.\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The last hidden states:\n",
    "            ``[batch_size x sent_length x max_word_length x embedding_size]``\n",
    "        \"\"\"\n",
    "        # Pass inputs through embedding layer.\n",
    "        inputs_emb = self.embedding(inputs)\n",
    "        # inputs_emb: ``[batch_size x sent_length x max_word_length x embedding_size]``\n",
    "        # Apply dropout to embeddings.\n",
    "        if self.embedding_dropout:\n",
    "            inputs_emb = self.embedding_dropout(inputs_emb)\n",
    "\n",
    "        return inputs_emb\n",
    "\n",
    "    '''@staticmethod\n",
    "    def cl_opts(group) -> None:\n",
    "        \"\"\"Define command-line options specific to this model.\"\"\"\n",
    "        group.add_argument(\n",
    "            \"--char-embedding-size\",\n",
    "            type=int,\n",
    "            default=50,\n",
    "            help=\"\"\"The dimension of the character embedding layer.\n",
    "            The default is 50.\"\"\"\n",
    "        )\n",
    "        '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Character-level CNN for genrating word features from kernels.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_chars : int\n",
    "        The number of characters in the vocabularly, i.e. the input size.\n",
    "    channels : int\n",
    "        The number of convolution channels.\n",
    "    kernel_size : int, optional (default: 3)\n",
    "        The size of the kernels.\n",
    "    padding : int, optional (default: 2)\n",
    "        The padding applied before the convolutional layer.\n",
    "    dropout : float, optional (default: 0.)\n",
    "        The dropout probability for the embedding layer.\n",
    "    embedding_size : int, optional (default: 50)\n",
    "        The size of the embedding layer.\n",
    "    padding_idx : int, optional (default: 0)\n",
    "        The id of the character using for padding.\n",
    "    Attributes\n",
    "    ----------\n",
    "    n_chars : int\n",
    "        The number of characters in the vocabularly, i.e. the input size.\n",
    "    char_embedding : torch.nn.Embedding\n",
    "        The character embedding layer.\n",
    "    cnn : torch.nn.Conv1d\n",
    "        The convolution layer.\n",
    "    output_size : int\n",
    "        The dimension of output.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 n_chars: int,\n",
    "                 channels: int,\n",
    "                 kernel_size: int = 3,\n",
    "                 embedding_size: int = 50,\n",
    "                 padding: int = 2,\n",
    "                 padding_idx: int = 0,\n",
    "                 dropout: float = 0.) -> None:\n",
    "        super(CharCNN, self).__init__()\n",
    "\n",
    "        self.n_chars = n_chars\n",
    "\n",
    "        # Character embedding layer.\n",
    "        self.char_embedding = CharEmbedding(n_chars, embedding_size,\n",
    "                                            dropout=dropout,\n",
    "                                            padding_idx=padding_idx)\n",
    "\n",
    "        # Convolutional layer.\n",
    "        self.cnn = Conv1d(embedding_size, \n",
    "                          channels, \n",
    "                          kernel_size, \n",
    "                          padding=padding)\n",
    "\n",
    "        self.output_size = channels\n",
    "\n",
    "    def forward(self,\n",
    "                inputs: torch.Tensor,\n",
    "                lengths: torch.Tensor,\n",
    "                indices: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Make a forward pass through the network.\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : torch.Tensor\n",
    "            Tensor of shape ``[sent_length x max_word_length]``.\n",
    "        lengths : torch.Tensor\n",
    "            The length of each word ``[sent_length]``.\n",
    "        indices : torch.Tensor\n",
    "            Sorted indices that we can recover the unsorted final hidden\n",
    "            states.\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The word features:\n",
    "            ``[sent_length x channels]``\n",
    "        \"\"\"\n",
    "        # Pass inputs through embedding layer.\n",
    "        inputs_emb = self.char_embedding(inputs).permute(0, 2, 1)\n",
    "        # inputs_emb: ``[sent_length x embedding_size x max_word_length ]``\n",
    "\n",
    "        # Run embeddings through convolution layer.\n",
    "        output = self.cnn(inputs_emb)\n",
    "        # output: ``[sent_length x channels x out_length]``\n",
    "        # ``out_length`` is a function of the ``max_word_length``,\n",
    "        # ``kernel_size``, and ``padding``.\n",
    "\n",
    "        # Apply max pooling across each word.\n",
    "        output, _ = torch.max(output, 2)\n",
    "        # output: ``[sent_length x channels]``\n",
    "\n",
    "        # Unsort the words.\n",
    "        output = unsort(output, indices)\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def cl_opts(parser: argparse.ArgumentParser, require=True) -> None:\n",
    "        # pylint: disable=unused-argument\n",
    "        \"\"\"Define command-line options specific to this model.\"\"\"\n",
    "        group = parser.add_argument_group(\"Character CNN options\")\n",
    "        CharEmbedding.cl_opts(group)\n",
    "        group.add_argument(\n",
    "            \"--cnn-channels\",\n",
    "            type=int,\n",
    "            default=30,\n",
    "            help=\"\"\"Number of convolutional channels. Default is 30.\"\"\"\n",
    "        )\n",
    "        group.add_argument(\n",
    "            \"--cnn-padding\",\n",
    "            type=int,\n",
    "            default=2,\n",
    "            help=\"\"\"Padding applied before CNN layer. Default is 2.\"\"\"\n",
    "        )\n",
    "        group.add_argument(\n",
    "            \"--cnn-kernel-size\",\n",
    "            type=int,\n",
    "            default=3,\n",
    "            help=\"\"\"Kernel size of the convolutions. Default is 3.\"\"\"\n",
    "        )\n",
    "    '''\n",
    "    @classmethod\n",
    "    def cl_init(cls, opts: argparse.Namespace, vocab: Vocab):\n",
    "        \"\"\"Initialize an instance of this model from command-line options.\"\"\"\n",
    "        return cls(\n",
    "            vocab.n_chars,\n",
    "            opts.cnn_channels,\n",
    "            kernel_size=opts.cnn_kernel_size,\n",
    "            padding=opts.cnn_padding,\n",
    "            dropout=opts.dropout,\n",
    "            embedding_size=opts.char_embedding_size)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_charCNN = CharCNN(26, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_BiLSTM_Encoder(nn.module):\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def forward(self):\n",
    "        return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM_LSTM_CRF_MY(nn.Module):\n",
    "    \"\"\"LM_LSTM_CRF model\n",
    "\n",
    "    args:\n",
    "        tagset_size: size of label set\n",
    "        char_size: size of char dictionary\n",
    "        char_dim: size of char embedding\n",
    "        char_hidden_dim: size of char-level lstm hidden dim\n",
    "        char_rnn_layers: number of char-level lstm layers\n",
    "        embedding_dim: size of word embedding\n",
    "        word_hidden_dim: size of word-level blstm hidden dim\n",
    "        word_rnn_layers: number of word-level lstm layers\n",
    "        vocab_size: size of word dictionary\n",
    "        dropout_ratio: dropout ratio\n",
    "        large_CRF: use CRF_L or not, refer model.crf.CRF_L and model.crf.CRF_S for more details\n",
    "        if_highway: use highway layers or not\n",
    "        in_doc_words: number of words that occurred in the corpus (used for language model prediction)\n",
    "        highway_layers: number of highway layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tagset_size, char_size, char_dim, char_hidden_dim, char_rnn_layers, embedding_dim, word_hidden_dim, word_rnn_layers, vocab_size, dropout_ratio, large_CRF=True, if_highway = False, in_doc_words = 2, highway_layers = 1):\n",
    "\n",
    "        super(LM_LSTM_CRF_MY, self).__init__()\n",
    "        self.char_dim = char_dim\n",
    "        self.char_hidden_dim = char_hidden_dim\n",
    "        self.char_size = char_size\n",
    "        self.word_dim = embedding_dim\n",
    "        self.word_hidden_dim = word_hidden_dim\n",
    "        self.word_size = vocab_size\n",
    "        self.if_highway = if_highway\n",
    "\n",
    "        self.char_embeds = nn.Embedding(char_size, char_dim)\n",
    "        self.forw_char_lstm = nn.LSTM(char_dim, char_hidden_dim, num_layers=char_rnn_layers, bidirectional=False, dropout=dropout_ratio)\n",
    "        self.back_char_lstm = nn.LSTM(char_dim, char_hidden_dim, num_layers=char_rnn_layers, bidirectional=False, dropout=dropout_ratio)\n",
    "        self.char_rnn_layers = char_rnn_layers\n",
    "\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.word_lstm = nn.LSTM(embedding_dim + char_hidden_dim * 2, word_hidden_dim // 2, num_layers=word_rnn_layers, bidirectional=True, dropout=dropout_ratio)\n",
    "\n",
    "        self.word_rnn_layers = word_rnn_layers\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout_ratio)\n",
    "\n",
    "        self.tagset_size = tagset_size\n",
    "        if large_CRF:\n",
    "            self.crf = crf.CRF_L(word_hidden_dim, tagset_size)\n",
    "        else:\n",
    "            self.crf = crf.CRF_S(word_hidden_dim, tagset_size)\n",
    "\n",
    "        if if_highway:\n",
    "            self.forw2char = highway.hw(char_hidden_dim, num_layers=highway_layers, dropout_ratio=dropout_ratio)\n",
    "            self.back2char = highway.hw(char_hidden_dim, num_layers=highway_layers, dropout_ratio=dropout_ratio)\n",
    "            self.forw2word = highway.hw(char_hidden_dim, num_layers=highway_layers, dropout_ratio=dropout_ratio)\n",
    "            self.back2word = highway.hw(char_hidden_dim, num_layers=highway_layers, dropout_ratio=dropout_ratio)\n",
    "            self.fb2char = highway.hw(2 * char_hidden_dim, num_layers=highway_layers, dropout_ratio=dropout_ratio)\n",
    "\n",
    "        self.char_pre_train_out = nn.Linear(char_hidden_dim, char_size)\n",
    "        self.word_pre_train_out = nn.Linear(char_hidden_dim, in_doc_words)\n",
    "\n",
    "        self.batch_size = 1\n",
    "        self.word_seq_length = 1\n",
    "\n",
    "    def set_batch_size(self, bsize):\n",
    "        \"\"\"\n",
    "        set batch size\n",
    "        \"\"\"\n",
    "        self.batch_size = bsize\n",
    "\n",
    "    def set_batch_seq_size(self, sentence):\n",
    "        \"\"\"\n",
    "        set batch size and sequence length\n",
    "        \"\"\"\n",
    "        tmp = sentence.size()\n",
    "        self.word_seq_length = tmp[0]\n",
    "        self.batch_size = tmp[1]\n",
    "\n",
    "    def rand_init_embedding(self):\n",
    "        \"\"\"\n",
    "        random initialize char-level embedding\n",
    "        \"\"\"\n",
    "        utils.init_embedding(self.char_embeds.weight)\n",
    "\n",
    "    def load_pretrained_word_embedding(self, pre_word_embeddings):\n",
    "        \"\"\"\n",
    "        load pre-trained word embedding\n",
    "\n",
    "        args:\n",
    "            pre_word_embeddings (self.word_size, self.word_dim) : pre-trained embedding\n",
    "        \"\"\"\n",
    "        assert (pre_word_embeddings.size()[1] == self.word_dim)\n",
    "        self.word_embeds.weight = nn.Parameter(pre_word_embeddings)\n",
    "\n",
    "    def rand_init(self, init_char_embedding=True, init_word_embedding=False):\n",
    "        \"\"\"\n",
    "        random initialization\n",
    "\n",
    "        args:\n",
    "            init_char_embedding: random initialize char embedding or not\n",
    "            init_word_embedding: random initialize word embedding or not\n",
    "        \"\"\"\n",
    "\n",
    "        if init_char_embedding:\n",
    "            utils.init_embedding(self.char_embeds.weight)\n",
    "        if init_word_embedding:\n",
    "            utils.init_embedding(self.word_embeds.weight)\n",
    "        if self.if_highway:\n",
    "            self.forw2char.rand_init()\n",
    "            self.back2char.rand_init()\n",
    "            self.forw2word.rand_init()\n",
    "            self.back2word.rand_init()\n",
    "            self.fb2char.rand_init()\n",
    "        utils.init_lstm(self.forw_char_lstm)\n",
    "        utils.init_lstm(self.back_char_lstm)\n",
    "        utils.init_lstm(self.word_lstm)\n",
    "        utils.init_linear(self.char_pre_train_out)\n",
    "        utils.init_linear(self.word_pre_train_out)\n",
    "        self.crf.rand_init()\n",
    "\n",
    "    def word_pre_train_forward(self, sentence, position, hidden=None):\n",
    "        \"\"\"\n",
    "        output of forward language model\n",
    "\n",
    "        args:\n",
    "            sentence (char_seq_len, batch_size): char-level representation of sentence\n",
    "            position (word_seq_len, batch_size): position of blank space in char-level representation of sentence\n",
    "            hidden: initial hidden state\n",
    "\n",
    "        return:\n",
    "            language model output (word_seq_len, in_doc_word), hidden\n",
    "        \"\"\"\n",
    "\n",
    "        embeds = self.char_embeds(sentence)\n",
    "        d_embeds = self.dropout(embeds)\n",
    "        lstm_out, hidden = self.forw_char_lstm(d_embeds)\n",
    "\n",
    "        tmpsize = position.size()\n",
    "        position = position.unsqueeze(2).expand(tmpsize[0], tmpsize[1], self.char_hidden_dim)\n",
    "        select_lstm_out = torch.gather(lstm_out, 0, position)\n",
    "        d_lstm_out = self.dropout(select_lstm_out).view(-1, self.char_hidden_dim)\n",
    "\n",
    "        if self.if_highway:\n",
    "            char_out = self.forw2word(d_lstm_out)\n",
    "            d_char_out = self.dropout(char_out)\n",
    "        else:\n",
    "            d_char_out = d_lstm_out\n",
    "\n",
    "        pre_score = self.word_pre_train_out(d_char_out)\n",
    "        return pre_score, hidden\n",
    "\n",
    "    def word_pre_train_backward(self, sentence, position, hidden=None):\n",
    "        \"\"\"\n",
    "        output of backward language model\n",
    "\n",
    "        args:\n",
    "            sentence (char_seq_len, batch_size): char-level representation of sentence (inverse order)\n",
    "            position (word_seq_len, batch_size): position of blank space in inversed char-level representation of sentence\n",
    "            hidden: initial hidden state\n",
    "\n",
    "        return:\n",
    "            language model output (word_seq_len, in_doc_word), hidden\n",
    "        \"\"\"\n",
    "        embeds = self.char_embeds(sentence)\n",
    "        d_embeds = self.dropout(embeds)\n",
    "        lstm_out, hidden = self.back_char_lstm(d_embeds)\n",
    "\n",
    "        tmpsize = position.size()\n",
    "        position = position.unsqueeze(2).expand(tmpsize[0], tmpsize[1], self.char_hidden_dim)\n",
    "        select_lstm_out = torch.gather(lstm_out, 0, position)\n",
    "        d_lstm_out = self.dropout(select_lstm_out).view(-1, self.char_hidden_dim)\n",
    "\n",
    "        if self.if_highway:\n",
    "            char_out = self.back2word(d_lstm_out)\n",
    "            d_char_out = self.dropout(char_out)\n",
    "        else:\n",
    "            d_char_out = d_lstm_out\n",
    "\n",
    "        pre_score = self.word_pre_train_out(d_char_out)\n",
    "        return pre_score, hidden\n",
    "\n",
    "    def forward(self, forw_sentence, forw_position, back_sentence, back_position, word_seq, hidden=None):\n",
    "        '''\n",
    "        args:\n",
    "            forw_sentence (char_seq_len, batch_size) : char-level representation of sentence\n",
    "            forw_position (word_seq_len, batch_size) : position of blank space in char-level representation of sentence\n",
    "            back_sentence (char_seq_len, batch_size) : char-level representation of sentence (inverse order)\n",
    "            back_position (word_seq_len, batch_size) : position of blank space in inversed char-level representation of sentence\n",
    "            word_seq (word_seq_len, batch_size) : word-level representation of sentence\n",
    "            hidden: initial hidden state\n",
    "\n",
    "        return:\n",
    "            crf output (word_seq_len, batch_size, tag_size, tag_size), hidden\n",
    "        '''\n",
    "\n",
    "        self.set_batch_seq_size(forw_position)\n",
    "\n",
    "        #embedding layer\n",
    "        forw_emb = self.char_embeds(forw_sentence)\n",
    "        back_emb = self.char_embeds(back_sentence)\n",
    "\n",
    "        #dropout\n",
    "        d_f_emb = self.dropout(forw_emb)\n",
    "        d_b_emb = self.dropout(back_emb)\n",
    "\n",
    "        #forward the whole sequence\n",
    "        forw_lstm_out, _ = self.forw_char_lstm(d_f_emb) #seq_len_char * batch * char_hidden_dim\n",
    "\n",
    "        back_lstm_out, _ = self.back_char_lstm(d_b_emb) #seq_len_char * batch * char_hidden_dim\n",
    "\n",
    "        #select predict point\n",
    "        forw_position = forw_position.unsqueeze(2).expand(self.word_seq_length, self.batch_size, self.char_hidden_dim)\n",
    "        select_forw_lstm_out = torch.gather(forw_lstm_out, 0, forw_position)\n",
    "\n",
    "        back_position = back_position.unsqueeze(2).expand(self.word_seq_length, self.batch_size, self.char_hidden_dim)\n",
    "        select_back_lstm_out = torch.gather(back_lstm_out, 0, back_position)\n",
    "\n",
    "        fb_lstm_out = self.dropout(torch.cat((select_forw_lstm_out, select_back_lstm_out), dim=2))\n",
    "        if self.if_highway:\n",
    "            char_out = self.fb2char(fb_lstm_out)\n",
    "            d_char_out = self.dropout(char_out)\n",
    "        else:\n",
    "            d_char_out = fb_lstm_out\n",
    "\n",
    "        #word\n",
    "        word_emb = self.word_embeds(word_seq)\n",
    "        d_word_emb = self.dropout(word_emb)\n",
    "\n",
    "        #combine: concatenating word_embedding with char_embedding\n",
    "        word_input = torch.cat((d_word_emb, d_char_out), dim = 2)\n",
    "\n",
    "        #word level lstm\n",
    "        lstm_out, _ = self.word_lstm(word_input)\n",
    "        d_lstm_out = self.dropout(lstm_out)\n",
    "\n",
    "        #convert to crf\n",
    "        crf_out = self.crf(d_lstm_out)\n",
    "        crf_out = crf_out.view(self.word_seq_length, self.batch_size, self.tagset_size, self.tagset_size)\n",
    "\n",
    "        return crf_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
